# MYREC File Format

The .myrec file format is a proprietary, undocumented, and unsupported VR recording format from ENGAGE. In it contains everything that the ENGAGE client needs to re-render a recording. Fortunately for those who study human behavior, this recording contains quite a view streams of data in high temporal fidelity. The recording includes position and rotation data for each participant, as well as IFX interactions, avatar choices, audio, and button events.

## File structure

The .myrec file is a ZIP file archive that contains several compressed files within it. This file structure is consistent across all .myrec files that I've encountered from June to December 2021. Once unzipped recursively, the file structure is:

-   `count.txt`
-   `master` (ZIP)
    -   `master.txt`
-   `stream0` (ZIP)
    -   `audioothers.mp3`
    -   `events.txt`
    -   `stream.txt`
-   `stream1` (ZIP)
-   ...

Each file is described below.

## File Contents

### count.txt

This file is a one-line file in what appears to be key-value pairs. Pairs are separated by a semicolon, and keys and values are separated by a pipe.

-   `streamCount`: the number of `streamN` archives in the top-level archive, where `N` is a non-negative integer.
-   `sceneID`: the environment in which the recording takes place.

### master.txt

This file gives information about the recording as a whole. For example, there is some information about the timing of the different `streamN` files. Information about the recorder's and participants' avatars is also stored here. While its file suffix is `.txt`, it is in fact a JSON file. There are too many entries to be described here.

### audioothers.mp3

This file is an mp3 file recording the spoken audio. Presumably, based on its name, it only captures audio spoken by people who were not recording, but this has not been verified directly. This file only exists in streams in which someone was speaking. In order to create an audio track for the entire file, one needs to create a space for silence for the duration of each stream without this file.

### events.txt

This file mainly deals with changes that aren't necessarily tied to an avatar. It is also a JSON file despite its suffix. Much of the activity in this file is related to IFX motion and usage. Other than hints given by the names in the keys, it is not understood what these values refer to specifically.

### stream.txt

This file is what appears to be a custom format storing the values of several variables for several users over a number of frames (140, in 2021). Breaking down each level from largest granularity to smallest:

**Users** are demarcated by one or multiple leading `>` characters. Each successive `>` increments the user's ID by one. For example, if users 1, 2, 5, and 6 are in the recording, then the `>` characters will be distributed as follows: `>` (1's data) `>` (2's data) `>>>` (5's data) `>` (6's data). Note that at the point in the file with user data, the user ID is equal to the number of `>`s that precede it in the entire file.

**Variables** are demarcated with a name and one `|` character at the beginning, and one `;` character at the end. Variable names follow the format `<name><type>x`. Here, `<type>` can be `int` for integer, `flo` for float, or `v3` for a 3D vector type.

The spatial variables we used had a `<name>` following the convention `<tracked_point>(Positions|Rotations)`. The tracked points included `AvaRoot`, `Head`, `LeftHand`, `RightHand`, `LeftFoot`, `RightFoot`, and `Hip`. For all the data collected in the summer and fall 2021 studies, the values for the feet and the hips were unused and not meaningful. `AvaRoot` stands for Avatar Root, and it defined the coordinate transformation from the coordinate space for `Head`, `LeftHand`, and `RightHand` into the global coordinate system.

The only other variable we used was `LipSyncAverageflox`, which indicated the amount of an avatar's lip-flapping and was presumably based upon volume. There were several other values that are included in the recording but we have not used, such as `IFXScaleflox`, `(Left|Right)TriggerPressedintx`, `(Left|Right)HandPointingintx`, `(Left|Right)HandLaserPointerintx` `(Left|Right)HandWhiteboardingintx`, `TabletOutintx`, `AvatarEmotionStateintx`, `RaisingHandintx`, `Clappingintx`, `OutfitOverrideStateintx`, `UseSitTriggerOverridesintx` `IsAwayintx`, `IsInSitTriggerintx`, `(Left|Right)HandWhiteboardEmitterPositionsv3x`, `IfxPositionsv3x`, and `IfxRotationsv3x`.

**Samples** are demarcated with with infixed `|`. Any values that are equal to the previous value are not included, so several variables often are written `|0|||||...`.

**3D Vectors** are demarcated in a special way relative to other data types. The three elements of the vector are separated with infix `<` symbols, and changes propagate along dimension individually. For example, the value `|<2.345<|` means the y-value changed to 2.345, but the x- and z- values remained the same. Position and rotation vectors were written the same way; they only difference in the data itself is the variable name discussed above. Positions are interpreted as `X<Y<Z`, where axis conventions are used according to Unity (Y up, Z out, left-handed). Rotations are interpreted as `pitch<yaw<roll`, where angle conventions are also used according to Unity.

## Interpretations

### Root, Head, Hands / Physical, Virtual, and Visible Motion

ENGAGE separates out motion into four related channels. `AvaRoot` stands for Avatar Root, and it defines the coordinate transformation from the coordinate space for `Head`, `LeftHand`, and `RightHand` into the global coordinate system. What is particularly nice about ENGAGE's data being in this format is that one can distinguish between *physical*, *virtual*, and *visible* motion. 

With virtual reality, what is *visible* to others in the virtual environment is a result of two kinds of motion: *physical* and *virtual* motion.

- **Physical** motion is what it sounds like: it's motion in the physical [real] world. Moving one's real hand creates physical motion. Calculating the physical position and motion is just as easy as selecting just the `Head`, `LeftHand`, and `RightHand` columns.
- **Virtual** motion is motion that is caused by some process that doesn't directly represent the motion. For example, teleporting is a kind of virtual motion, and so is pressing a button and moving in the virtual world. The thing that caused that motion wasn't literally motion, it was something translated by the system. This is given by just the `AvaRoot` columns, because it's the same in ENGAGE across all the three other tracked points.
- **Visible** motion is the motion that is (usually) the vector sum of these two types of motion. This is the way ENGAGE does it, at least. It is the motion with the virtual world as its framing. To calculate the visible position using `dddr` and `ungage`, take the example of this code:
```r
read_tsv(file_path, col_types = list(Timestamp = col_time("%H:%M:%OS"))) %>%
  bundle(
    Root = "AvaRoot_Pos{v}",
    RootRot = "AvaRoot_Rot{ed}",
    PhysicalHead = "Head_Pos{v}",
    PhysicalHeadRot = "Head_Rot{ed}"
  ) %>%
  select(Timestamp, Root, RootRot, PhysicalHead, PhysicalHeadRot) %>%
  mutate(
    # convert to visible motion
    VisibleRawHead = Root + rotate(PhysicalHead, RootRot),
    VisibleRawHeadRot = rotate(PhysicalHeadRot, RootRot, as = "orientation")
  )
```

It should be noted this taxonomy is not complete. For example, it doesn't allow for describing alternative methods of motion, such as redirected walking or visuo-haptic illusions. Nevertheless, it is a useful description of motion when working with ENGAGE.

### others...

\[\[more could be written here --- a good threshold for smooth motion is 3.05 m/s\]\]
