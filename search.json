[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "motion-data-book",
    "section": "",
    "text": "Overview\n[What is this book for]\nPart of it is motion in general…\n\nThinking about space, scientifically\n\ngeospatial vs. human-spatial\npersonal space findings\n\nThinking about space, mathematically\nTidy spatial data with dddr\n\nwhy a separate library\n\nKey spatial operations\n\nPart of it is a technical guide…\nThe process is (in order):\n\nUnderstanding the .myrec file format\nUnderstanding the BIDS format\nConverting from .myrec to BIDS\nExample analysis"
  },
  {
    "objectID": "myrec.html#file-structure",
    "href": "myrec.html#file-structure",
    "title": "1  MYREC File Format",
    "section": "1.1 File structure",
    "text": "1.1 File structure\nThe .myrec file is a ZIP file archive that contains several compressed files within it. This file structure is consistent across all .myrec files that I’ve encountered from June to December 2021. Once unzipped recursively, the file structure is:\n\ncount.txt\nmaster (ZIP)\n\nmaster.txt\n\nstream0 (ZIP)\n\naudioothers.mp3\nevents.txt\nstream.txt\n\nstream1 (ZIP)\n…\n\nEach file is described below."
  },
  {
    "objectID": "myrec.html#file-contents",
    "href": "myrec.html#file-contents",
    "title": "1  MYREC File Format",
    "section": "1.2 File Contents",
    "text": "1.2 File Contents\n\n1.2.1 count.txt\nThis file is a one-line file in what appears to be key-value pairs. Pairs are separated by a semicolon, and keys and values are separated by a pipe.\n\nstreamCount: the number of streamN archives in the top-level archive, where N is a non-negative integer.\nsceneID: the environment in which the recording takes place.\n\n\n\n1.2.2 master.txt\nThis file gives information about the recording as a whole. For example, there is some information about the timing of the different streamN files. Information about the recorder’s and participants’ avatars is also stored here. While its file suffix is .txt, it is in fact a JSON file. There are too many entries to be described here.\n\n\n1.2.3 audioothers.mp3\nThis file is an mp3 file recording the spoken audio. Presumably, based on its name, it only captures audio spoken by people who were not recording, but this has not been verified directly. This file only exists in streams in which someone was speaking. In order to create an audio track for the entire file, one needs to create a space for silence for the duration of each stream without this file.\n\n\n1.2.4 events.txt\nThis file mainly deals with changes that aren’t necessarily tied to an avatar. It is also a JSON file despite its suffix. Much of the activity in this file is related to IFX motion and usage. Other than hints given by the names in the keys, it is not understood what these values refer to specifically.\n\n\n1.2.5 stream.txt\nThis file is what appears to be a custom format storing the values of several variables for several users over a number of frames (140, in 2021). Breaking down each level from largest granularity to smallest:\nUsers are demarcated by one or multiple leading &gt; characters. Each successive &gt; increments the user’s ID by one. For example, if users 1, 2, 5, and 6 are in the recording, then the &gt; characters will be distributed as follows: &gt; (1’s data) &gt; (2’s data) &gt;&gt;&gt; (5’s data) &gt; (6’s data). Note that at the point in the file with user data, the user ID is equal to the number of &gt;s that precede it in the entire file.\nVariables are demarcated with a name and one | character at the beginning, and one ; character at the end. Variable names follow the format &lt;name&gt;&lt;type&gt;x. Here, &lt;type&gt; can be int for integer, flo for float, or v3 for a 3D vector type.\nThe spatial variables we used had a &lt;name&gt; following the convention &lt;tracked_point&gt;(Positions|Rotations). The tracked points included AvaRoot, Head, LeftHand, RightHand, LeftFoot, RightFoot, and Hip. For all the data collected in the summer and fall 2021 studies, the values for the feet and the hips were unused and not meaningful. AvaRoot stands for Avatar Root, and it defined the coordinate transformation from the coordinate space for Head, LeftHand, and RightHand into the global coordinate system.\nThe only other variable we used was LipSyncAverageflox, which indicated the amount of an avatar’s lip-flapping and was presumably based upon volume. There were several other values that are included in the recording but we have not used, such as IFXScaleflox, (Left|Right)TriggerPressedintx, (Left|Right)HandPointingintx, (Left|Right)HandLaserPointerintx (Left|Right)HandWhiteboardingintx, TabletOutintx, AvatarEmotionStateintx, RaisingHandintx, Clappingintx, OutfitOverrideStateintx, UseSitTriggerOverridesintx IsAwayintx, IsInSitTriggerintx, (Left|Right)HandWhiteboardEmitterPositionsv3x, IfxPositionsv3x, and IfxRotationsv3x.\nSamples are demarcated with with infixed |. Any values that are equal to the previous value are not included, so several variables often are written |0|||||....\n3D Vectors are demarcated in a special way relative to other data types. The three elements of the vector are separated with infix &lt; symbols, and changes propagate along dimension individually. For example, the value |&lt;2.345&lt;| means the y-value changed to 2.345, but the x- and z- values remained the same. Position and rotation vectors were written the same way; they only difference in the data itself is the variable name discussed above. Positions are interpreted as X&lt;Y&lt;Z, where axis conventions are used according to Unity (Y up, Z out, left-handed). Rotations are interpreted as pitch&lt;yaw&lt;roll, where angle conventions are also used according to Unity."
  },
  {
    "objectID": "myrec.html#conventions",
    "href": "myrec.html#conventions",
    "title": "1  MYREC File Format",
    "section": "1.3 Conventions",
    "text": "1.3 Conventions\n(more could be written here — a good threshold for smooth motion is 3.05 m/s)"
  },
  {
    "objectID": "bids.html",
    "href": "bids.html",
    "title": "2  BIDS (Brain Imaging Data Structure)",
    "section": "",
    "text": "link: https://bids-specification.readthedocs.io/en/stable/\nalso: https://docs.google.com/document/d/1iaaLKgWjK5pcISD1MVxHKexB3PZWfE2aAC5HF_pCZWo/edit"
  },
  {
    "objectID": "converting-myrec-bids.html#extracting-myrec",
    "href": "converting-myrec-bids.html#extracting-myrec",
    "title": "3  Converting MYREC to BIDS",
    "section": "3.1 Extracting Myrec",
    "text": "3.1 Extracting Myrec\n(see the Rmd file I sent over a long while ago)"
  },
  {
    "objectID": "converting-myrec-bids.html#forming-to-bids",
    "href": "converting-myrec-bids.html#forming-to-bids",
    "title": "3  Converting MYREC to BIDS",
    "section": "3.2 Forming to BIDS",
    "text": "3.2 Forming to BIDS"
  },
  {
    "objectID": "examples.html#libraries",
    "href": "examples.html#libraries",
    "title": "4  Examples of Social Analysis",
    "section": "4.1 libraries",
    "text": "4.1 libraries\nsynsyn (https://github.com/markromanmiller/synsyn) dddr"
  },
  {
    "objectID": "examples.html#total-motion-over-time",
    "href": "examples.html#total-motion-over-time",
    "title": "4  Examples of Social Analysis",
    "section": "4.2 total motion over time",
    "text": "4.2 total motion over time"
  },
  {
    "objectID": "examples.html#average-distance-between-people",
    "href": "examples.html#average-distance-between-people",
    "title": "4  Examples of Social Analysis",
    "section": "4.3 average distance between people",
    "text": "4.3 average distance between people"
  },
  {
    "objectID": "examples.html#heatmap",
    "href": "examples.html#heatmap",
    "title": "4  Examples of Social Analysis",
    "section": "4.4 Heatmap",
    "text": "4.4 Heatmap"
  },
  {
    "objectID": "examples.html#gaze-distribution",
    "href": "examples.html#gaze-distribution",
    "title": "4  Examples of Social Analysis",
    "section": "4.5 Gaze distribution",
    "text": "4.5 Gaze distribution"
  },
  {
    "objectID": "examples.html#mutual-gaze",
    "href": "examples.html#mutual-gaze",
    "title": "4  Examples of Social Analysis",
    "section": "4.6 Mutual gaze",
    "text": "4.6 Mutual gaze\n# Load libraries\nlibrary(tidyverse)\ndevtools::load_all(\"~/thesis/multiverse/\")\ndevtools::load_all(\"~/work/dddr\")\ndevtools::load_all(\"~/thesis/rbids/\")\ndevtools::load_all(\"~/thesis/synsyn/\")\n\n# load BIDS dataset\nbd &lt;- bids(\"/media/mark/mrm-thesis-files/virtual-summer/bids-standard/vhil-2021-summer/\")\n\n# let dddr know that we're working with Unity's axis and angles conventions.\nset_dddr_semantics(axes = semantics_axes_unity, angles = semantics_angles_unity)\n\n# Take the file, read it, and get virtual position of root, and physical position of head, and find visible position.\npreprocess &lt;- function(file_path, session_id, participant_id, ...) {\n  # exactly one file per person\n  read_tsv(file_path, col_types = list(Timestamp = col_time(\"%H:%M:%OS\")), progress = F) %&gt;%\n    bundle(\n      Root = \"Root_Pos{v}\",\n      RootRot = \"Root_Rot{ed}\",\n      RawHead = \"Head_Pos{v}\",\n      RawHeadRot = \"Head_Rot{ed}\"\n    ) %&gt;%\n    select(Timestamp, Root, RootRot, RawHead, RawHeadRot) %&gt;%\n    mutate(session_id = session_id) %&gt;%\n    mutate(\n      # convert to visible motion\n      VisibleRawHead = Root + rotate(RawHead, RootRot),\n      VisibleRawHeadRot = rotate(RawHeadRot, RootRot, as = \"orientation\")\n    )\n}\n\n# take two participant's placement, and calculate whether they're looking at each other or if X is looking at Y.\nrelate_intrinsic_yp &lt;- function(px_data, py_data) {\n  joined &lt;- inner_join(px_data, py_data, by = c(\"Timestamp\"))\n\n  if (nrow(joined) == 0) {\n    return (NULL)\n  }\n\n  joined %&gt;%\n    #slice_sample(prop = 0.01) %&gt;%\n    mutate(\n      y_from_x = angle_between(VisibleRawHead.y - VisibleRawHead.x, vector3(0, 0, 1) %&gt;% rotate(VisibleRawHeadRot.x)) &lt; 15/180 * pi,\n      x_from_y = angle_between(VisibleRawHead.x - VisibleRawHead.y, vector3(0, 0, 1) %&gt;% rotate(VisibleRawHeadRot.y)) &lt; 15/180 * pi,\n      mutual = y_from_x & x_from_y\n    ) %&gt;%\n    select(\n      Timestamp, y_from_x, mutual\n    )\n}\n\n# This function takes several pairs' worth of data and collapses them to one timeframe for each person.\ngroup_summary_align_and_boolean &lt;- function(list_df) {\n  list_df %&gt;%\n    bind_rows() %&gt;%\n    unnest(result) %&gt;%\n    group_by(participant_id_x, Timestamp) %&gt;%\n    summarize(\n      y_from_x = any(y_from_x),\n      mutual = any(mutual),\n      .groups = \"drop_last\"\n    ) %&gt;%\n    summarize(\n      social_attention = mean(y_from_x, na.rm = T),\n      mutual_gaze = mean(mutual, na.rm = T),\n      total_frames = n(),\n      na_social_attention_frames = sum(is.na(y_from_x)),\n      na_mutual_gaze_frames = sum(is.na(mutual)),\n      .groups = \"drop\"\n    ) %&gt;%\n    group_nest(.key = \"result\")\n}\n\n# Now that the functions are defined, run them on the BIDS data\nresults &lt;- bd %&gt;%\n  bids_motion() %&gt;%\n  #filter(session_id %&gt;% endsWith(\"section1\")) %&gt;%\n  #filter(session_id == \"week7section8\") %&gt;%\n  summarize_motion_pairs(\n    relate = relate_intrinsic_yp,\n    preprocess = preprocess,\n    group_summary = group_summary_align_and_boolean,\n    ordered = T,\n    #head = 5,\n    progress = T\n  ) %&gt;%\n  unnest(result)\n\n# Output the file with a timestamp to make it unique.\noutput_file &lt;- paste0(\"results_\", strftime(Sys.time(), format = \"%y_%m_%d_%H%M%S\"), \".csv\")\ncat(\"Output file at\", output_file)\n\nresults %&gt;% write_csv(output_file)"
  }
]